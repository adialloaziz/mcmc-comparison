{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summer2 import CompartmentalModel\n",
    "from summer2.parameters import Parameter\n",
    "import arviz as az\n",
    "import Calibrate as cal\n",
    "import seaborn as sns\n",
    "from jax.scipy.stats import gaussian_kde\n",
    "from jax import lax\n",
    "\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pymc as pm\n",
    "from estival.wrappers import pymc as epm\n",
    "\n",
    "import numpyro\n",
    "from numpyro import infer\n",
    "from numpyro import distributions as dist\n",
    "from jax import random\n",
    "import pickle\n",
    "from scipy.special import kl_div, rel_entr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "\n",
    "    sir_model = CompartmentalModel([0.0,100.0],[\"S\",\"I\",\"R\"],[\"I\"])\n",
    "    sir_model.set_initial_population({\"S\": 999.0, \"I\": 1.0})\n",
    "    sir_model.add_infection_frequency_flow(\"infection\",Parameter(\"contact_rate\"),\"S\",\"I\")\n",
    "    sir_model.add_transition_flow(\"recovery\",Parameter(\"recovery_rate\"),\"I\",\"R\")\n",
    "\n",
    "    sir_model.request_output_for_flow(\"incidence\", \"infection\")\n",
    "    \n",
    "    return sir_model\n",
    "\n",
    "sir_model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"contact_rate\": 0.3,\n",
    "    \"recovery_rate\": 0.1\n",
    "}\n",
    "sir_model.run(parameters)\n",
    "res = sir_model.get_derived_outputs_df()\n",
    "# res['incidence'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample from a known distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "def sample_from_truncnorm(mean, std_dev, lower_bound, upper_bound, sample_size, name):\n",
    "    a = (lower_bound - mean) / std_dev\n",
    "    b = (upper_bound - mean) / std_dev\n",
    "    samples = truncnorm.rvs(a, b, loc=mean, scale=std_dev, size=sample_size)\n",
    "\n",
    "    return pd.DataFrame(samples, columns=[name])\n",
    "\n",
    "# samples = {\n",
    "#     \"contact_rate\":  pd.concat(\n",
    "#         [\n",
    "#             sample_from_truncnorm(0.07, 0.005, 0.15, 0.25, 5000, \"contact_rate\"),\n",
    "#             sample_from_truncnorm(0.3, 0.013, 0.25, 0.35, 10000, \"contact_rate\"),\n",
    "#         ],       \n",
    "#         ignore_index=True\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_div_threshold = 0.01\n",
    "import pickle\n",
    "\n",
    "\n",
    "# #Storing our sample\n",
    "# samples\n",
    "# with open(\"./true_sample_severe.pkl\", 'wb') as fp:\n",
    "        # pickle.dump(samples['contact_rate'], fp)\n",
    "\n",
    "#Load samples\n",
    "with open(\"./true_sample.pkl\", 'rb') as fp:\n",
    "       samples = pickle.load(fp)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(samples[\"contact_rate\"], fill=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run model forward (i.e. feed the samples to the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from estival.model import BayesianCompartmentalModel\n",
    "import estival.priors as esp\n",
    "import estival.targets as est\n",
    "from estival.sampling import tools as esamp\n",
    "\n",
    "\n",
    "priors = [\n",
    "    esp.UniformPrior(\"contact_rate\", [0.0, 0.1]),\n",
    "]\n",
    "targets = []\n",
    "bcm = BayesianCompartmentalModel(model=sir_model,priors=priors, targets=targets,parameters=parameters)\n",
    "samples_for_estival = [{\"contact_rate\": samples[\"contact_rate\"].iloc[i]} for i in range(len(samples[\"contact_rate\"]))]\n",
    "\n",
    "\n",
    "model_runs = esamp.model_results_for_samples(samples_for_estival, bcm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_runs.results['incidence'].plot(legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect the synthetic data and generate likelihood components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_times = list(range(10, 91, 10))\n",
    "data_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood_comps = {t: gaussian_kde(jnp.array(model_runs.results['incidence'].loc[t]), bw_method=0.01) for t in data_times}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check one likelihood component\n",
    "for t in data_times:\n",
    "    kde = likelihood_comps[t]\n",
    "    x_values = np.linspace(0, 50, 1000)\n",
    "    pdf_values = kde(x_values)\n",
    "    plt.plot(x_values, pdf_values)\n",
    "\n",
    "    model_runs.results['incidence'].loc[t].plot.hist(density=True, bins=50)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refit the model using the likelihood components derived from synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flat prior\n",
    "priors = [\n",
    "    esp.UniformPrior(\"contact_rate\", [0.1, 0.5]),\n",
    "]\n",
    "n_data_points = len(data_times)\n",
    "# Define a custom target using the likelihood components\n",
    "def make_eval_func(t):\n",
    "    def eval_func(modelled, obs, parameters, time_weights):\n",
    "        likelihood_comp = likelihood_comps[t](modelled) \n",
    "        likelihood_comp = jnp.max(jnp.array([likelihood_comp, jnp.array([1.e-300])]))  # to avoid zero values.\n",
    "        return jnp.log(likelihood_comp) / n_data_points\n",
    "\n",
    "    return eval_func\n",
    "\n",
    "targets = [est.CustomTarget(f\"likelihood_comp_{t}\", pd.Series([0.], index=[t]), make_eval_func(t), model_key='incidence') for t in data_times]\n",
    "\n",
    "refit_bcm = BayesianCompartmentalModel(model=sir_model,priors=priors, targets=targets,parameters=parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pymc sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chains = 4\n",
    "init_vals = []\n",
    "for c in range(chains):\n",
    "    init_vals.append({\"contact_rate\": np.random.uniform(0.01,0.6) })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDATA = dict()\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "Draws = [10000]*4 \n",
    "Sampler = [pm.sample_smc,pm.Metropolis, pm.DEMetropolis, pm.DEMetropolisZ]\n",
    "for sampler, draws in zip(Sampler, Draws):\n",
    "    results = cal.Single_analysis(sampler = sampler, \n",
    "            draws = draws,\n",
    "            chains=4,\n",
    "            cores = 4,\n",
    "            tune = 1000,\n",
    "            bcm_model = refit_bcm,\n",
    "            # initial_params = init_vals\n",
    ")\n",
    "            \n",
    "    results_df = pd.concat([results_df,results])\n",
    "\n",
    "\n",
    "\n",
    "results_df = results_df.reset_index(drop=True)\n",
    "\n",
    "# with open('./Results/Reverse_Ingineering/Exper_3_severe_trough.pkl', 'wb') as fp:\n",
    "#     pickle.dump(results_df, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NUTS sampling (Numpyro)\n",
    "\n",
    "We need to define quickly a numpyro compatible model. Here only the parameter \"contact_rate\" is involved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmodel():\n",
    "    sampled = {\"contact_rate\":numpyro.sample(\"contact_rate\", dist.Uniform(0.0,1.0))}# for k in refit_bcm.parameters}\n",
    "    ll = numpyro.factor(\"ll\", refit_bcm.loglikelihood(**sampled))\n",
    "\n",
    "#Initialisation\n",
    "# init_vals_nuts = {\"contact_rate\": jnp.full(4, 0.26) }\n",
    "\n",
    "#init_vals_nuts = {\"contact_rate\": jnp.array(np.random.uniform(0.,.6, 4)) }\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = infer.NUTS\n",
    "results = cal.Single_analysis(sampler = sampler, \n",
    "            draws = 10000,\n",
    "            chains=4,\n",
    "            cores = 4,\n",
    "            tune = 1000,\n",
    "            bcm_model = refit_bcm,\n",
    "            nmodel=nmodel,\n",
    "            # initial_params = init_vals_nuts\n",
    "\n",
    "    )\n",
    "results_df = pd.concat([results_df,results])\n",
    "results_df = results_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading previous results \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Results/Reverse_Ingineering/Multi_run_1.pkl', 'rb') as fp:\n",
    "    all_results = pickle.load(fp) #It's a dict\n",
    "\n",
    "res = pd.concat(all_results) #To a pd.DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Kullback-Leibler divergence against the known distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kullback_Leibler_div(all_results, true_sample):\n",
    "        # temp = pd.concat(all_results)\n",
    "        # temp[\"KL_div\"] = None #create a new column\n",
    "        true_sample = samples.to_numpy(dtype=np.float64)\n",
    "        true_sample = true_sample.reshape(-1) #Reshaping to a 1d array\n",
    "        true_sample = true_sample/np.sum(true_sample)\n",
    "        for sampler in all_results.keys():\n",
    "                df = all_results[sampler]\n",
    "                df[\"KL_div\"] = df[\"Rhat_max\"]#create a new column \n",
    "\n",
    "                for row in df.index:\n",
    "                        trace = df.Trace.loc[row]\n",
    "                        Predict_sample = np.array(trace.posterior.to_dataframe()[\"contact_rate\"].to_list())\n",
    "                        #Normalizing the distribution\n",
    "                        #We select only the last \"true_sample.size\" elements of the predicted\n",
    "                        #To ensure matching shape\n",
    "                        Predict_sample = Predict_sample[-true_sample.shape[0]:]\n",
    "                        Predict_sample = Predict_sample/np.sum(Predict_sample)\n",
    "\n",
    "                        df.at[row,\"KL_div\"] = np.sum(kl_div(true_sample,Predict_sample)).round(7)\n",
    "                all_results[sampler] = df #Updating \n",
    "        return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Kullback_Leibler_div(all_results,samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#____Selecting min KL_div for each sampler over 100 runs\n",
    "best_results = pd.DataFrame()\n",
    "for sampler in df.keys():\n",
    "    temp = df[sampler]\n",
    "    best_kldiv = temp.loc[[temp[\"KL_div\"].idxmin()]]\n",
    "    best_results = pd.concat([best_results,best_kldiv])\n",
    "\n",
    "best_results = best_results.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idata, sampler in zip(best_results.Trace, best_results.Sampler):\n",
    "    print(sampler)\n",
    "    az.plot_trace(idata,figsize=(9,4))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(18, 3))\n",
    "i = 0\n",
    "for sampler , idata in zip(best_results.Sampler,best_results.Trace):\n",
    "    ax = axes[i]\n",
    "    posterior_sample = idata.posterior.to_dataframe()['contact_rate'].to_list()\n",
    "    # plt.hist(samples[\"contact_rate\"],histtype='step', bins=50, density=True, label=\"true sample\")\n",
    "    # plt.hist(posterior_sample, bins=50, histtype='step',density=True, label=\"posterior by \"+ sampler)\n",
    "    sns.kdeplot(samples,ax = ax, fill=True, label=\"true sample\")\n",
    "    sns.kdeplot(posterior_sample,ax = ax, fill=True, label= sampler)\n",
    "    ax.legend(loc = \"upper left\")\n",
    "    i = i+1\n",
    "    # ax.set_xlabel(\"\")\n",
    "\n",
    "plt.suptitle(f\"Posterior by different MCMC samplers\", fontsize=12)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lls = esamp.likelihood_extras_for_idata(idata, refit_bcm)\n",
    "lls = esamp.likelihood_extras_for_samples(idata.posterior, refit_bcm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lls['logposterior'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lls['logposterior'].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_model_runs = esamp.model_results_for_samples(idata, refit_bcm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = posterior_model_runs.results['incidence']#[149].plot(legend=\"refit\")\n",
    "# model_runs.results['incidence'][149].plot(legend=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = pd.concat([D[0],D[1], D[2],D[3]], axis=1, join=\"outer\", ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.set_index(D.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_runs.results['incidence'][9].plot(label= \"true\", legend=True)\n",
    "D[15000].plot(label = \"fit\", legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summer2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
