{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (pytensor.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "from summer2 import CompartmentalModel\n",
    "from summer2.parameters import Parameter\n",
    "import arviz as az\n",
    "import Calibrate as cal\n",
    "import seaborn as sns\n",
    "from jax.scipy.stats import gaussian_kde\n",
    "from jax import lax\n",
    "\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pymc as pm\n",
    "from estival.wrappers import pymc as epm\n",
    "\n",
    "import numpyro\n",
    "from numpyro import infer\n",
    "from numpyro import distributions as dist\n",
    "from jax import random\n",
    "import pickle\n",
    "from scipy.special import kl_div, rel_entr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "\n",
    "    sir_model = CompartmentalModel([0.0,100.0],[\"S\",\"I\",\"R\"],[\"I\"])\n",
    "    sir_model.set_initial_population({\"S\": 999.0, \"I\": 1.0})\n",
    "    sir_model.add_infection_frequency_flow(\"infection\",Parameter(\"contact_rate\"),\"S\",\"I\")\n",
    "    sir_model.add_transition_flow(\"recovery\",Parameter(\"recovery_rate\"),\"I\",\"R\")\n",
    "\n",
    "    sir_model.request_output_for_flow(\"incidence\", \"infection\")\n",
    "    \n",
    "    return sir_model\n",
    "\n",
    "sir_model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"contact_rate\": 0.3,\n",
    "    \"recovery_rate\": 0.1\n",
    "}\n",
    "sir_model.run(parameters)\n",
    "res = sir_model.get_derived_outputs_df()\n",
    "# res['incidence'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample from a known distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "def sample_from_truncnorm(mean, std_dev, lower_bound, upper_bound, sample_size, name):\n",
    "    a = (lower_bound - mean) / std_dev\n",
    "    b = (upper_bound - mean) / std_dev\n",
    "    samples = truncnorm.rvs(a, b, loc=mean, scale=std_dev, size=sample_size)\n",
    "\n",
    "    return pd.DataFrame(samples, columns=[name])\n",
    "\n",
    "# samples = {\n",
    "#     \"contact_rate\":  pd.concat(\n",
    "#         [\n",
    "#             sample_from_truncnorm(0.07, 0.005, 0.15, 0.25, 5000, \"contact_rate\"),\n",
    "#             sample_from_truncnorm(0.3, 0.013, 0.25, 0.35, 10000, \"contact_rate\"),\n",
    "#         ],       \n",
    "#         ignore_index=True\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_div_threshold = 0.01\n",
    "import pickle\n",
    "\n",
    "\n",
    "# #Storing our sample\n",
    "# samples\n",
    "# with open(\"./true_sample_severe.pkl\", 'wb') as fp:\n",
    "        # pickle.dump(samples['contact_rate'], fp)\n",
    "\n",
    "#Load samples\n",
    "with open(\"./true_sample.pkl\", 'rb') as fp:\n",
    "       samples = pickle.load(fp)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(samples[\"contact_rate\"], fill=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run model forward (i.e. feed the samples to the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from estival.model import BayesianCompartmentalModel\n",
    "import estival.priors as esp\n",
    "import estival.targets as est\n",
    "from estival.sampling import tools as esamp\n",
    "\n",
    "\n",
    "priors = [\n",
    "    esp.UniformPrior(\"contact_rate\", [0.0, 0.1]),\n",
    "]\n",
    "targets = []\n",
    "bcm = BayesianCompartmentalModel(model=sir_model,priors=priors, targets=targets,parameters=parameters)\n",
    "samples_for_estival = [{\"contact_rate\": samples[\"contact_rate\"].iloc[i]} for i in range(len(samples[\"contact_rate\"]))]\n",
    "\n",
    "\n",
    "model_runs = esamp.model_results_for_samples(samples_for_estival, bcm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_runs.results['incidence'].plot(legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect the synthetic data and generate likelihood components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 20, 30, 40, 50, 60, 70, 80, 90]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_times = list(range(10, 91, 10))\n",
    "data_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood_comps = {t: gaussian_kde(jnp.array(model_runs.results['incidence'].loc[t]), bw_method=0.01) for t in data_times}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check one likelihood component\n",
    "for t in data_times:\n",
    "    kde = likelihood_comps[t]\n",
    "    x_values = np.linspace(0, 50, 1000)\n",
    "    pdf_values = kde(x_values)\n",
    "    plt.plot(x_values, pdf_values)\n",
    "\n",
    "    model_runs.results['incidence'].loc[t].plot.hist(density=True, bins=50)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refit the model using the likelihood components derived from synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flat prior\n",
    "priors = [\n",
    "    esp.UniformPrior(\"contact_rate\", [0.1, 0.5]),\n",
    "]\n",
    "n_data_points = len(data_times)\n",
    "# Define a custom target using the likelihood components\n",
    "def make_eval_func(t):\n",
    "    def eval_func(modelled, obs, parameters, time_weights):\n",
    "        likelihood_comp = likelihood_comps[t](modelled) \n",
    "        likelihood_comp = jnp.max(jnp.array([likelihood_comp, jnp.array([1.e-300])]))  # to avoid zero values.\n",
    "        return jnp.log(likelihood_comp) / n_data_points\n",
    "\n",
    "    return eval_func\n",
    "\n",
    "targets = [est.CustomTarget(f\"likelihood_comp_{t}\", pd.Series([0.], index=[t]), make_eval_func(t), model_key='incidence') for t in data_times]\n",
    "\n",
    "refit_bcm = BayesianCompartmentalModel(model=sir_model,priors=priors, targets=targets,parameters=parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pymc sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chains = 4\n",
    "init_vals = []\n",
    "for c in range(chains):\n",
    "    init_vals.append({\"contact_rate\": np.random.uniform(0.01,0.6) })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDATA = dict()\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "Draws = [10000]*4 \n",
    "Sampler = [pm.sample_smc,pm.Metropolis, pm.DEMetropolis, pm.DEMetropolisZ]\n",
    "for sampler, draws in zip(Sampler, Draws):\n",
    "    results = cal.Single_analysis(sampler = sampler, \n",
    "            draws = draws,\n",
    "            chains=4,\n",
    "            cores = 4,\n",
    "            tune = 1000,\n",
    "            bcm_model = refit_bcm,\n",
    "            # initial_params = init_vals\n",
    ")\n",
    "            \n",
    "    results_df = pd.concat([results_df,results])\n",
    "\n",
    "\n",
    "\n",
    "results_df = results_df.reset_index(drop=True)\n",
    "\n",
    "# with open('./Results/Reverse_Ingineering/Exper_3_severe_trough.pkl', 'wb') as fp:\n",
    "#     pickle.dump(results_df, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NUTS sampling (Numpyro)\n",
    "\n",
    "We need to define quickly a numpyro compatible model. Here only the parameter \"contact_rate\" is involved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmodel():\n",
    "    sampled = {\"contact_rate\":numpyro.sample(\"contact_rate\", dist.Uniform(0.0,1.0))}# for k in refit_bcm.parameters}\n",
    "    ll = numpyro.factor(\"ll\", refit_bcm.loglikelihood(**sampled))\n",
    "\n",
    "#Initialisation\n",
    "# init_vals_nuts = {\"contact_rate\": jnp.full(4, 0.26) }\n",
    "\n",
    "#init_vals_nuts = {\"contact_rate\": jnp.array(np.random.uniform(0.,.6, 4)) }\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = infer.NUTS\n",
    "results = cal.Single_analysis(sampler = sampler, \n",
    "            draws = 10000,\n",
    "            chains=4,\n",
    "            cores = 4,\n",
    "            tune = 1000,\n",
    "            bcm_model = refit_bcm,\n",
    "            nmodel=nmodel,\n",
    "            # initial_params = init_vals_nuts\n",
    "\n",
    "    )\n",
    "results_df = pd.concat([results_df,results])\n",
    "results_df = results_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading previous results \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./Results/Reverse_Ingineering/Exper_3_severe_trough.pkl\", 'rb') as fp:\n",
    "    all_results = pickle.load(fp) #It's a dict\n",
    "\n",
    "# res = pd.concat(all_results) #To a pd.DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing the Relative ESS\n",
    "all_results[\"Rel_Ess\"] = all_results['Min_Ess'].astype(float)/(all_results[\"Draws\"].astype(float)*all_results['Chains'].astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sampler</th>\n",
       "      <th>Draws</th>\n",
       "      <th>Chains</th>\n",
       "      <th>Tune</th>\n",
       "      <th>Time</th>\n",
       "      <th>Mean_ESS</th>\n",
       "      <th>Min_Ess</th>\n",
       "      <th>Ess_per_sec</th>\n",
       "      <th>Mean_Rhat</th>\n",
       "      <th>Rhat_max</th>\n",
       "      <th>Trace</th>\n",
       "      <th>Run</th>\n",
       "      <th>Rel_Ess</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sample_smc</td>\n",
       "      <td>10000</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>232.296019</td>\n",
       "      <td>1471.063252</td>\n",
       "      <td>1471.063252</td>\n",
       "      <td>6.332710</td>\n",
       "      <td>1.002123</td>\n",
       "      <td>1.002123</td>\n",
       "      <td>(posterior, sample_stats)</td>\n",
       "      <td>sample_smc\\nDraws=10000\\nTune=1000\\nChains=4</td>\n",
       "      <td>0.036777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Metropolis</td>\n",
       "      <td>10000</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>116.406171</td>\n",
       "      <td>6.509680</td>\n",
       "      <td>6.509680</td>\n",
       "      <td>0.055922</td>\n",
       "      <td>1.626626</td>\n",
       "      <td>1.626626</td>\n",
       "      <td>(posterior, sample_stats)</td>\n",
       "      <td>Metropolis\\nDraws=10000\\nTune=1000\\nChains=4</td>\n",
       "      <td>0.000163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DEMetropolis</td>\n",
       "      <td>10000</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>134.645172</td>\n",
       "      <td>8363.011864</td>\n",
       "      <td>8363.011864</td>\n",
       "      <td>62.111487</td>\n",
       "      <td>1.000479</td>\n",
       "      <td>1.000479</td>\n",
       "      <td>(posterior, sample_stats)</td>\n",
       "      <td>DEMetropolis\\nDraws=10000\\nTune=1000\\nChains=4</td>\n",
       "      <td>0.209075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DEMetropolisZ</td>\n",
       "      <td>10000</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>113.132659</td>\n",
       "      <td>9360.410016</td>\n",
       "      <td>9360.410016</td>\n",
       "      <td>82.738354</td>\n",
       "      <td>1.000308</td>\n",
       "      <td>1.000308</td>\n",
       "      <td>(posterior, sample_stats)</td>\n",
       "      <td>DEMetropolisZ\\nDraws=10000\\nTune=1000\\nChains=4</td>\n",
       "      <td>0.234010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NUTS</td>\n",
       "      <td>10000</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>404.128990</td>\n",
       "      <td>5.229346</td>\n",
       "      <td>5.229346</td>\n",
       "      <td>0.012940</td>\n",
       "      <td>2.098957</td>\n",
       "      <td>2.098957</td>\n",
       "      <td>(posterior, log_likelihood, sample_stats, obse...</td>\n",
       "      <td>NUTS\\nDraws=10000\\nTune=1000\\nChains=4</td>\n",
       "      <td>0.000131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Sampler  Draws  Chains  Tune        Time     Mean_ESS      Min_Ess  \\\n",
       "0     sample_smc  10000       4  1000  232.296019  1471.063252  1471.063252   \n",
       "1     Metropolis  10000       4  1000  116.406171     6.509680     6.509680   \n",
       "2   DEMetropolis  10000       4  1000  134.645172  8363.011864  8363.011864   \n",
       "3  DEMetropolisZ  10000       4  1000  113.132659  9360.410016  9360.410016   \n",
       "4           NUTS  10000       4  1000  404.128990     5.229346     5.229346   \n",
       "\n",
       "   Ess_per_sec  Mean_Rhat  Rhat_max  \\\n",
       "0     6.332710   1.002123  1.002123   \n",
       "1     0.055922   1.626626  1.626626   \n",
       "2    62.111487   1.000479  1.000479   \n",
       "3    82.738354   1.000308  1.000308   \n",
       "4     0.012940   2.098957  2.098957   \n",
       "\n",
       "                                               Trace  \\\n",
       "0                          (posterior, sample_stats)   \n",
       "1                          (posterior, sample_stats)   \n",
       "2                          (posterior, sample_stats)   \n",
       "3                          (posterior, sample_stats)   \n",
       "4  (posterior, log_likelihood, sample_stats, obse...   \n",
       "\n",
       "                                               Run   Rel_Ess  \n",
       "0     sample_smc\\nDraws=10000\\nTune=1000\\nChains=4  0.036777  \n",
       "1     Metropolis\\nDraws=10000\\nTune=1000\\nChains=4  0.000163  \n",
       "2   DEMetropolis\\nDraws=10000\\nTune=1000\\nChains=4  0.209075  \n",
       "3  DEMetropolisZ\\nDraws=10000\\nTune=1000\\nChains=4  0.234010  \n",
       "4           NUTS\\nDraws=10000\\nTune=1000\\nChains=4  0.000131  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Kullback-Leibler divergence against the known distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kullback_Leibler_div(all_results, true_sample):\n",
    "        # temp = pd.concat(all_results)\n",
    "        # temp[\"KL_div\"] = None #create a new column\n",
    "        true_sample = samples.to_numpy(dtype=np.float64)\n",
    "        true_sample = true_sample.reshape(-1) #Reshaping to a 1d array\n",
    "        true_sample = true_sample/np.sum(true_sample)\n",
    "        for sampler in all_results.keys():\n",
    "                df = all_results[sampler]\n",
    "                df[\"KL_div\"] = df[\"Rhat_max\"]#create a new column \n",
    "\n",
    "                for row in df.index:\n",
    "                        trace = df.Trace.loc[row]\n",
    "                        Predict_sample = np.array(trace.posterior.to_dataframe()[\"contact_rate\"].to_list())\n",
    "                        #Normalizing the distribution\n",
    "                        #We select only the last \"true_sample.size\" elements of the predicted\n",
    "                        #To ensure matching shape\n",
    "                        Predict_sample = Predict_sample[-true_sample.shape[0]:]\n",
    "                        Predict_sample = Predict_sample/np.sum(Predict_sample)\n",
    "\n",
    "                        df.at[row,\"KL_div\"] = np.sum(kl_div(true_sample,Predict_sample)).round(7)\n",
    "                all_results[sampler] = df #Updating \n",
    "        return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Kullback_Leibler_div(all_results,samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#____Selecting min KL_div for each sampler over 100 runs\n",
    "best_results = pd.DataFrame()\n",
    "for sampler in df.keys():\n",
    "    temp = df[sampler]\n",
    "    best_kldiv = temp.loc[[temp[\"KL_div\"].idxmin()]]\n",
    "    best_results = pd.concat([best_results,best_kldiv])\n",
    "\n",
    "best_results = best_results.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idata, sampler in zip(best_results.Trace, best_results.Sampler):\n",
    "    print(sampler)\n",
    "    az.plot_trace(idata,figsize=(9,4))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(18, 3))\n",
    "i = 0\n",
    "for sampler , idata in zip(best_results.Sampler,best_results.Trace):\n",
    "    ax = axes[i]\n",
    "    posterior_sample = idata.posterior.to_dataframe()['contact_rate'].to_list()\n",
    "    # plt.hist(samples[\"contact_rate\"],histtype='step', bins=50, density=True, label=\"true sample\")\n",
    "    # plt.hist(posterior_sample, bins=50, histtype='step',density=True, label=\"posterior by \"+ sampler)\n",
    "    sns.kdeplot(samples,ax = ax, fill=True, label=\"true sample\")\n",
    "    sns.kdeplot(posterior_sample,ax = ax, fill=True, label= sampler)\n",
    "    ax.legend(loc = \"upper left\")\n",
    "    i = i+1\n",
    "    # ax.set_xlabel(\"\")\n",
    "\n",
    "plt.suptitle(f\"Posterior by different MCMC samplers\", fontsize=12)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lls = esamp.likelihood_extras_for_idata(idata, refit_bcm)\n",
    "lls = esamp.likelihood_extras_for_samples(idata.posterior, refit_bcm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lls['logposterior'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lls['logposterior'].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_model_runs = esamp.model_results_for_samples(idata, refit_bcm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = posterior_model_runs.results['incidence']#[149].plot(legend=\"refit\")\n",
    "# model_runs.results['incidence'][149].plot(legend=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = pd.concat([D[0],D[1], D[2],D[3]], axis=1, join=\"outer\", ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.set_index(D.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_runs.results['incidence'][9].plot(label= \"true\", legend=True)\n",
    "D[15000].plot(label = \"fit\", legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summer2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
