{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summer2 import CompartmentalModel\n",
    "from summer2.parameters import Parameter\n",
    "import arviz as az\n",
    "import Calibrate as cal\n",
    "import seaborn as sns\n",
    "from jax.scipy.stats import gaussian_kde\n",
    "from jax import lax\n",
    "\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pymc as pm\n",
    "from estival.wrappers import pymc as epm\n",
    "\n",
    "import numpyro\n",
    "from numpyro import infer\n",
    "from numpyro import distributions as dist\n",
    "from jax import random\n",
    "import pickle\n",
    "from scipy.special import kl_div, rel_entr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "\n",
    "    sir_model = CompartmentalModel([0.0,100.0],[\"S\",\"I\",\"R\"],[\"I\"])\n",
    "    sir_model.set_initial_population({\"S\": 999.0, \"I\": 1.0})\n",
    "    sir_model.add_infection_frequency_flow(\"infection\",Parameter(\"contact_rate\"),\"S\",\"I\")\n",
    "    sir_model.add_transition_flow(\"recovery\",Parameter(\"recovery_rate\"),\"I\",\"R\")\n",
    "\n",
    "    sir_model.request_output_for_flow(\"incidence\", \"infection\")\n",
    "    \n",
    "    return sir_model\n",
    "\n",
    "sir_model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"contact_rate\": 0.3,\n",
    "    \"recovery_rate\": 0.1\n",
    "}\n",
    "sir_model.run(parameters)\n",
    "res = sir_model.get_derived_outputs_df()\n",
    "# res['incidence'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample from a known distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "def sample_from_truncnorm(mean, std_dev, lower_bound, upper_bound, sample_size, name):\n",
    "    a = (lower_bound - mean) / std_dev\n",
    "    b = (upper_bound - mean) / std_dev\n",
    "    samples = truncnorm.rvs(a, b, loc=mean, scale=std_dev, size=sample_size)\n",
    "\n",
    "    return pd.DataFrame(samples, columns=[name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a new sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {\n",
    "    \"contact_rate\":  pd.concat(\n",
    "        [\n",
    "            sample_from_truncnorm(0.07, 0.005, 0.15, 0.25, 5000, \"contact_rate\"),\n",
    "            sample_from_truncnorm(0.3, 0.013, 0.25, 0.35, 10000, \"contact_rate\"),\n",
    "        ],       \n",
    "        ignore_index=True\n",
    "    )\n",
    "}\n",
    "# #Storing our samples for later use\n",
    "# samples\n",
    "# with open(\"./true_sample_severe.pkl\", 'wb') as fp:\n",
    "        # pickle.dump(samples['contact_rate'], fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or upload the samples used in the paper mainly for result compararison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load samples used in the paper\n",
    "with open(\"./true_sample.pkl\", 'rb') as fp:\n",
    "       samples = pickle.load(fp)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(samples[\"contact_rate\"], fill=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run model forward (i.e. feed the samples to the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from estival.model import BayesianCompartmentalModel\n",
    "import estival.priors as esp\n",
    "import estival.targets as est\n",
    "from estival.sampling import tools as esamp\n",
    "\n",
    "\n",
    "priors = [\n",
    "    esp.UniformPrior(\"contact_rate\", [0.0, 0.1]),\n",
    "]\n",
    "targets = []\n",
    "bcm = BayesianCompartmentalModel(model=sir_model,priors=priors, targets=targets,parameters=parameters)\n",
    "samples_for_estival = [{\"contact_rate\": samples[\"contact_rate\"].iloc[i]} for i in range(len(samples[\"contact_rate\"]))]\n",
    "\n",
    "\n",
    "model_runs = esamp.model_results_for_samples(samples_for_estival, bcm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_runs.results['incidence'].plot(legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect the synthetic data and generate likelihood components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_times = list(range(10, 91, 10))\n",
    "likelihood_comps = {t: gaussian_kde(jnp.array(model_runs.results['incidence'].loc[t]), bw_method=0.01) for t in data_times}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check one likelihood component\n",
    "for t in data_times:\n",
    "    kde = likelihood_comps[t]\n",
    "    x_values = np.linspace(0, 50, 1000)\n",
    "    pdf_values = kde(x_values)\n",
    "    plt.plot(x_values, pdf_values)\n",
    "\n",
    "    model_runs.results['incidence'].loc[t].plot.hist(density=True, bins=50)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refit the model using the likelihood components derived from synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flat prior\n",
    "priors = [\n",
    "    esp.UniformPrior(\"contact_rate\", [0.1, 0.5]),\n",
    "]\n",
    "n_data_points = len(data_times)\n",
    "# Define a custom target using the likelihood components\n",
    "def make_eval_func(t):\n",
    "    def eval_func(modelled, obs, parameters, time_weights):\n",
    "        likelihood_comp = likelihood_comps[t](modelled) \n",
    "        likelihood_comp = jnp.max(jnp.array([likelihood_comp, jnp.array([1.e-300])]))  # to avoid zero values.\n",
    "        return jnp.log(likelihood_comp) / n_data_points\n",
    "\n",
    "    return eval_func\n",
    "\n",
    "targets = [est.CustomTarget(f\"likelihood_comp_{t}\", pd.Series([0.], index=[t]), make_eval_func(t), model_key='incidence') for t in data_times]\n",
    "\n",
    "refit_bcm = BayesianCompartmentalModel(model=sir_model,priors=priors, targets=targets,parameters=parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling \n",
    "Runing sampling algorithms over the refited model in order to recover the initial samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pymc sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chains = 4\n",
    "init_vals = []\n",
    "for c in range(chains):\n",
    "    init_vals.append({\"contact_rate\": np.random.uniform(0.01,0.6) })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDATA = dict()\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "Draws = [1000]*4 # Increase the number of draws for better accuracy\n",
    "Sampler = [pm.sample_smc,pm.Metropolis, pm.DEMetropolis, pm.DEMetropolisZ]\n",
    "for sampler, draws in zip(Sampler, Draws):\n",
    "    results = cal.Single_analysis(sampler = sampler, \n",
    "            draws = draws,\n",
    "            chains=4,\n",
    "            cores = 4,\n",
    "            tune = 100,\n",
    "            bcm_model = refit_bcm,\n",
    "            # initial_params = init_vals\n",
    ")\n",
    "            \n",
    "    results_df = pd.concat([results_df,results])\n",
    "\n",
    "\n",
    "\n",
    "results_df = results_df.reset_index(drop=True)\n",
    "\n",
    "# with open('./Results/Reverse_Ingineering/Exper_3_severe_trough.pkl', 'wb') as fp:\n",
    "#     pickle.dump(results_df, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NUTS sampling (Numpyro)\n",
    "\n",
    "We need to define quickly a numpyro compatible model. Here only the parameter \"contact_rate\" is involved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmodel():\n",
    "    sampled = {\"contact_rate\":numpyro.sample(\"contact_rate\", dist.Uniform(0.0,1.0))}# for k in refit_bcm.parameters}\n",
    "    ll = numpyro.factor(\"ll\", refit_bcm.loglikelihood(**sampled))\n",
    "\n",
    "#Initialisation\n",
    "# init_vals_nuts = {\"contact_rate\": jnp.full(4, 0.26) }\n",
    "\n",
    "#init_vals_nuts = {\"contact_rate\": jnp.array(np.random.uniform(0.,.6, 4)) }\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = infer.NUTS\n",
    "results = cal.Single_analysis(sampler = sampler, \n",
    "            draws = 1000, #Increase this number for better accuracy\n",
    "            chains=4,\n",
    "            cores = 4,\n",
    "            tune = 100,\n",
    "            bcm_model = refit_bcm,\n",
    "            nmodel=nmodel,\n",
    "            # initial_params = init_vals_nuts\n",
    "\n",
    "    )\n",
    "results_df = pd.concat([results_df,results])\n",
    "results_df = results_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal.plot_comparison_bars(results_df=results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multirun analyse\n",
    "\n",
    "Please refer to the fitting_simulation_script.py for the 100 runs of each alogirthm.\n",
    "\n",
    "Here we try to establish statistics from theses runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading previous results \n",
    "We upload the results obtained from the pyhton script mentioned above.\n",
    "Make sure to locate properly yours. Here is an example. This file is too heavy to be uploaded in the Github repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./Results/Reverse_Ingineering/Multi_run_all.pkl\", 'rb') as fp:\n",
    "    all_results = pickle.load(fp) #It's a dict\n",
    "\n",
    "\n",
    "all_results = pd.read_pickle(\"./Results/Reverse_Ingineering/Multi_run_all.pkl\") #It's a dict\n",
    "\n",
    "res = pd.concat(all_results) #To a pd.DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing the Relative ESS\n",
    "res[\"Rel_Ess\"] = res['Min_Ess'].astype(float)/(res[\"Draws\"].astype(float)*res['Chains'].astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_mean, prcnt_succ = cal.group_summary(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Kullback-Leibler divergence against the known distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kl_div_threshold = 0.01\n",
    "def Kullback_Leibler_div(all_results, true_sample):\n",
    "        true_sample = samples.to_numpy(dtype=np.float64)\n",
    "        true_sample = true_sample.reshape(-1) #Reshaping to a 1d array\n",
    "        true_sample = true_sample/np.sum(true_sample)\n",
    "        for sampler in all_results.keys():\n",
    "                df = all_results[sampler]\n",
    "                df[\"KL_div\"] = df[\"Rhat_max\"]#create a new column \n",
    "\n",
    "                for row in df.index:\n",
    "                        trace = df.Trace.loc[row]\n",
    "                        Predict_sample = np.array(trace.posterior.to_dataframe()[\"contact_rate\"].to_list())\n",
    "                        #Normalizing the distribution\n",
    "                        #We select only the last \"true_sample.size\" elements of the predicted\n",
    "                        #To ensure matching shape\n",
    "                        Predict_sample = Predict_sample[-true_sample.shape[0]:]\n",
    "                        Predict_sample = Predict_sample/np.sum(Predict_sample)\n",
    "\n",
    "                        df.at[row,\"KL_div\"] = np.sum(kl_div(true_sample,Predict_sample)).round(7)\n",
    "                all_results[sampler] = df #Updating \n",
    "        return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Kullback_Leibler_div(all_results,samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting min KL_div for each sampler over 100 runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#____Selecting min KL_div for each sampler over 100 runs\n",
    "best_results = pd.DataFrame()\n",
    "for sampler in df.keys():\n",
    "    temp = df[sampler]\n",
    "    best_kldiv = temp.loc[[temp[\"KL_div\"].idxmin()]]\n",
    "    best_results = pd.concat([best_results,best_kldiv])\n",
    "\n",
    "best_results = best_results.reset_index(drop=True)\n",
    "#Computing the Relative ESS\n",
    "best_results[\"Rel_Ess\"] = best_results['Min_Ess'].astype(float)/(best_results[\"Draws\"].astype(float)*best_results['Chains'].astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bars(results_df):\n",
    "    pd.options.plotting.backend = \"matplotlib\"\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(10, 5))\n",
    "    ax = axes[0]\n",
    "    results_df.plot.bar(y=\"Ess_per_sec\", x=\"Sampler\", ax=ax, legend=False)\n",
    "    ax.set_title(\"Ess_per_sec\")\n",
    "    ax.set_xlabel(\"\")\n",
    "    labels = ax.get_xticklabels()\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    ax = axes[1]\n",
    "    results_df.plot.bar(y=\"Rel_Ess\", x=\"Sampler\", ax=ax, legend=False)\n",
    "    ax.set_title(\"Relative ESS\")\n",
    "    ax.set_xlabel(\"\")\n",
    "    labels = ax.get_xticklabels()\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "    ax = axes[2]\n",
    "    results_df.plot.bar(y=\"Rhat_max\", x=\"Sampler\", ax=ax, legend=False)\n",
    "    ax.set_title(r\"Max $\\hat{R}$\")\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylim(1)\n",
    "    labels = ax.get_xticklabels()\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "    ax = axes[3]\n",
    "    results_df.plot.bar(y=\"KL_div\", x=\"Sampler\", ax=ax, legend=False)\n",
    "    ax.set_title(\"Min KL_div\")\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylim(0.005)\n",
    "    labels = ax.get_xticklabels()\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "    plt.suptitle(f\"Sampler Comparison\", fontsize=12)\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bars(best_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ploting the trace for the best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idata, sampler in zip(best_results.Trace, best_results.Sampler):\n",
    "    print(sampler)\n",
    "    az.plot_trace(idata,figsize=(9,4))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior distribution Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 5, figsize=(18, 3))\n",
    "i = 0\n",
    "for sampler , idata in zip(best_results.Sampler,best_results.Trace):\n",
    "    ax = axes[i]\n",
    "    posterior_sample = idata.posterior.to_dataframe()['contact_rate'].to_list()\n",
    "    # plt.hist(samples[\"contact_rate\"],histtype='step', bins=50, density=True, label=\"true sample\")\n",
    "    # plt.hist(posterior_sample, bins=50, histtype='step',density=True, label=\"posterior by \"+ sampler)\n",
    "    sns.kdeplot(samples,ax = ax, fill=True, label=\"true sample\")\n",
    "    sns.kdeplot(posterior_sample,ax = ax, fill=True, label= sampler)\n",
    "    ax.legend(loc = \"upper left\")\n",
    "    i = i+1\n",
    "    # ax.set_xlabel(\"\")\n",
    "\n",
    "plt.suptitle(f\"Posterior by different MCMC samplers\", fontsize=12)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lls = esamp.likelihood_extras_for_idata(idata, refit_bcm)\n",
    "lls = esamp.likelihood_extras_for_samples(idata.posterior, refit_bcm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lls['logposterior'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lls['logposterior'].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_model_runs = esamp.model_results_for_samples(idata, refit_bcm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = posterior_model_runs.results['incidence']#[149].plot(legend=\"refit\")\n",
    "# model_runs.results['incidence'][149].plot(legend=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = pd.concat([D[0],D[1], D[2],D[3]], axis=1, join=\"outer\", ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.set_index(D.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_runs.results['incidence'][9].plot(label= \"true\", legend=True)\n",
    "D[15000].plot(label = \"fit\", legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Landscape Analysis of the posterior distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#____We generate a sample of parameters which will allow us to study the landscape of our logposterior\n",
    "#____Using Latin Hypercube Sampling to do so\n",
    "\n",
    "#!pip install pflacco #Installing the pflacco package for Landscape Analysis\n",
    "#---------Some features---------------\n",
    "from pflacco.classical_ela_features import *\n",
    "from pflacco.classical_ela_features import calculate_ela_distribution\n",
    "from pflacco.misc_features import calculate_fitness_distance_correlation\n",
    "from pflacco.local_optima_network_features import compute_local_optima_network, calculate_lon_features\n",
    "\n",
    "#__To___create_a_initial____sample\n",
    "from pflacco.sampling import create_initial_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Loglikelihood(x):\n",
    "    #x can be a vector where parameters values represents the components\n",
    "    #if calibrating many parameters; Here only the contact rate is calibrated.\n",
    "\n",
    "    params = {\"contact_rate\": x, \"recovery_rate\": 0.1}\n",
    "    return float(refit_bcm.loglikelihood(**params))\n",
    "def negative(f, *args):\n",
    "    \"\"\"\n",
    "    Wrap a positive function such that a minimizable version is returned instead\n",
    "    Args:\n",
    "    f: The callable to wrap\n",
    "    \"\"\"\n",
    "    def _reflected(*args):\n",
    "        return float(0.0 - f(*args))\n",
    "\n",
    "    return _reflected(*args)\n",
    "def obj_func (x):\n",
    "    return negative(Loglikelihood,*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inital sample using latin hyper cube sampling\n",
    "n = 2**6\n",
    "X = create_initial_sample(1, n, sample_type = \"sobol\")\n",
    "\n",
    "Y = X.apply(lambda x: obj_func(x),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ela_distr = calculate_ela_distribution(X, Y.values.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ela_distr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summer2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
