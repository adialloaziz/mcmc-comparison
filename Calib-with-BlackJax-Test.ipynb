{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we try to sample using Black Jax which include HMC/NUTS/Tempered Hamiltonian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "# %pip install jax==0.4.28\n",
    "# !pip install blackjax\n",
    "# !pip install estival\n",
    "# !pip install PyYaml\n",
    "# !pip install pandas\n",
    "# !pip install nbformat --upgrade\n",
    "\n",
    "# !pip install jaxlib==0.4.28\n",
    "# !pip install pymc\n",
    "# !pip install --upgrade --force-reinstall mingw\n",
    "# %pip --upgrade m2w64-toolchain\n",
    "# %pip install multiprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install summerepi2==1.3.6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import blackjax\n",
    "#import blackjax.smc.resampling as resampling\n",
    "import pandas as pd\n",
    "import jax\n",
    "import pymc as pm\n",
    "import time as time\n",
    "\n",
    "from datetime import date\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining tagets and prior with our summer2 model in a BayesianCompartmentalModel (BCM)\n",
    "from estival.model import BayesianCompartmentalModel\n",
    "\n",
    "from calibs_utilities import get_all_priors, get_targets, load_data\n",
    "\n",
    "from models.models import model1 #All the models we design for the test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing our taget and prior data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.plotting.backend = \"plotly\"\n",
    "output_labels = {\"index\": \"time\", \"value\": \"number infectious\"}\n",
    "\n",
    "targets_yml = './data/target_yml.yml'\n",
    "targets = load_data(targets_yml)\n",
    "targets_data = targets['active_cases']\n",
    "\n",
    "\n",
    "\n",
    "#Names of parameters and their ranges for the priors definitions\n",
    "params = {\n",
    "    \"contact_rate\": (0.0,2.0),\n",
    "    \"recovery_rate\": (0.0,1.0)\n",
    "\n",
    "}\n",
    "\n",
    "targets = get_targets(targets_yml)\n",
    "priors = get_all_priors(params)\n",
    "\n",
    "targets_data.plot(kind=\"scatter\",labels=output_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a BCM model with estival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = model1()\n",
    "default_params = {\n",
    "    \"contact_rate\": 0.5,\n",
    "    \"recovery_rate\": 0.0,\n",
    "    #\"active_cases_dispersion\": 0.5,\n",
    "}\n",
    "bcm = BayesianCompartmentalModel(model_1, default_params,priors, targets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogLikelihood definition from the bcm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_tuple = namedtuple(\"model_params\", [\"contact_rate\", \"recovery_rate\"]) #,typename=np.float64)\n",
    "\n",
    "\n",
    "#Defining a callable Loglikelihood from the bcm model\n",
    "def Loglikehood(params_tuple):\n",
    "        params = {'contact_rate': params_tuple.contact_rate, 'recovery_rate': params_tuple.recovery_rate}  #, 'cdr': 0.2}\n",
    "        val = bcm.loglikelihood(**params)\n",
    "        return val.astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initial parameters for the chains\n",
    "# For now we initialise with default chosen constant values for all chains\n",
    "# We can later experiment a Uniform initialisation\n",
    "import tensorflow_probability.substrates.jax as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "def init_param_fn(seed): #Permet d'initialiser les parametres pour chaque chaine \n",
    "    \"\"\"\n",
    "    initialize recovery_rate & contact_rate for the chains \n",
    "    \"\"\"\n",
    "    key1, key2 = jax.random.split(seed, 2)\n",
    "    return params_tuple(\n",
    "    #    contact_rate = jnp.ones(n_chains)*0.15,\n",
    "    #     recovery_rate = jnp.ones(n_chains)*0.05,\n",
    "    contact_rate = tfd.Uniform(0, 1).sample(seed=key1).astype(np.float64),\n",
    "    recovery_rate = tfd.Uniform(0, 0.5).sample(seed=key2).astype(np.float64),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use n_chains chains for sampling\n",
    "rng_key = jax.random.key(int(date.today().strftime(\"%Y%m%d\")))\n",
    "n_chains = 2\n",
    "\n",
    "rng_key, init_key, warmup_key = jax.random.split(rng_key, 3)\n",
    "init_keys = jax.random.split(init_key, n_chains)\n",
    "init_params = jax.vmap(init_param_fn)(init_keys)\n",
    "\n",
    "\n",
    "#Checking result with jax vmap to map the function to multiple args\n",
    "jax.vmap(Loglikehood)(init_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we use blackjaxâ€™s window adaption algorithm to get NUTS kernel and initial states. Window adaption algorithm will automatically configure inverse_mass_matrix and step size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#rng_key = jax.random.key(int(date.today().strftime(\"%Y%m%d\")))\n",
    "rng_key, warmup_key = jax.random.split(rng_key, 2)\n",
    "\n",
    "hmc_parameters = dict(num_integration_steps = 10)\n",
    "\n",
    "warmup = blackjax.window_adaptation(blackjax.hmc, logdensity_fn = Loglikehood,target_acceptance_rate = 0.6,**hmc_parameters)\n",
    "\n",
    "# we use n_chains chains for sampling\n",
    "@jax.vmap\n",
    "def call_warmup(seed, param):\n",
    "    (initial_states, tuned_params), _ = warmup.run(seed, param, 1000)\n",
    "    return initial_states, tuned_params\n",
    "\n",
    "\n",
    "warmup_keys = jax.random.split(warmup_key, n_chains)\n",
    "\n",
    "initial_states, tuned_params = jax.jit(call_warmup)(warmup_keys, init_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling wiht HMC from BlackJax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An inference loop for multiple chains sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_loop_multiple_chains(\n",
    "    rng_key, initial_states, tuned_params, log_prob_fn, num_samples, num_chains\n",
    "):\n",
    "    kernel = blackjax.hmc.build_kernel()\n",
    "    @jax.jit  #Comment to assess the performance\n",
    "    def step_fn(key, state, **params):\n",
    "        return kernel(key, state, log_prob_fn, **params)\n",
    "\n",
    "    def one_step(states, rng_key):\n",
    "        keys = jax.random.split(rng_key, num_chains)\n",
    "        states, infos = jax.vmap(step_fn)(keys, states, **tuned_params)\n",
    "        return states, (states, infos)\n",
    "\n",
    "    keys = jax.random.split(rng_key, num_samples)\n",
    "    _, (states, infos) = jax.lax.scan(one_step, initial_states, keys)\n",
    "\n",
    "    return (states, infos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling process\n",
    "\n",
    "Slow process. Has to be run in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_samples = 5000\n",
    "rng_key, sample_key = jax.random.split(rng_key)\n",
    "\n",
    "states, infos = inference_loop_multiple_chains(\n",
    "        rng_key = sample_key,\n",
    "        initial_states =  initial_states,\n",
    "        tuned_params = tuned_params,\n",
    "        log_prob_fn = Loglikehood,\n",
    "        num_samples = n_samples,\n",
    "        num_chains = n_chains,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derived the trace from the outputs of our sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arviz_trace_from_states(states, info, burn_in=0):\n",
    "    position = states.position\n",
    "    if isinstance(position, jax.Array):  # if states.position is array of samples\n",
    "        position = dict(samples=position)\n",
    "    else:\n",
    "        try:\n",
    "            position = position._asdict()\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "    samples = {}\n",
    "    for param in position.keys():\n",
    "        ndims = len(position[param].shape)\n",
    "        if ndims >= 2:\n",
    "            samples[param] = jnp.swapaxes(position[param], 0, 1)[\n",
    "                :, burn_in:\n",
    "            ]  # swap n_samples and n_chains\n",
    "            divergence = jnp.swapaxes(info.is_divergent[burn_in:], 0, 1)\n",
    "\n",
    "        if ndims == 1:\n",
    "            divergence = info.is_divergent\n",
    "            samples[param] = position[param]\n",
    "\n",
    "    trace_posterior = az.convert_to_inference_data(samples)\n",
    "    trace_sample_stats = az.convert_to_inference_data(\n",
    "        {\"diverging\": divergence}, group=\"sample_stats\"\n",
    "    )\n",
    "    trace = az.concat(trace_posterior, trace_sample_stats)\n",
    "    return trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make arviz trace from states\n",
    "trace = arviz_trace_from_states(states, infos, burn_in=0)\n",
    "summ_df = az.summary(trace)\n",
    "summ_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import multiprocess as mp\n",
    "import platform\n",
    "\n",
    "if platform.system() != \"Windows\":\n",
    "    \n",
    "    mp.set_start_method('forkserver')\n",
    "\n",
    "\n",
    "from estival.wrappers import pymc as epm\n",
    "from pymc.sampling import jax as jpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    variables = epm.use_model(bcm, include_ll=True)\n",
    "    # The log-posterior value can also be output, but may incur additional overhead\n",
    "    # Use jacobian=False to get the unwarped value (ie just the 'native' density of the priors\n",
    "    # without transformation correction factors)\n",
    "    variables = pm.Deterministic(\"logp\", model.logp(jacobian=False))\n",
    "    \n",
    "    # Now call a sampler using the variables from use_model\n",
    "    idata_HCM = jpm._sample_blackjax_nuts(model,\n",
    "                                        draws=100,\n",
    "                                        tune = 100,\n",
    "                                        chains=4, \n",
    "                                        chain_method='parallel',\n",
    "                                        target_accept=0.5,\n",
    "                                        progressbar=False,\n",
    "                                        random_seed=1,\n",
    "                                        initial_points={\"contact_rate\": 0.9, \"recovery_rate\": 0.4},\n",
    "                                        nuts_kwargs = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc.sampling import jax\n",
    "\n",
    "jax._sample_blackjax_nuts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
